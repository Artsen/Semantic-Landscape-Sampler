"""Projection and clustering utilities for semantic landscapes.

Classes:
    ProjectionResult: Holds paired 2D/3D coordinate matrices generated by dimensionality reduction.
    ClusterResult: Encapsulates clustering outputs including labels, probabilities, similarities, and centroids.

Functions:
    compute_umap(...): Produce normalised 2D and 3D coordinates from embedding matrices.
    run_hdbscan(...)/run_kmeans(...): Perform clustering with probabilistic fallbacks.
    cluster_with_fallback(...): Choose an appropriate clustering strategy based on dataset size.
    build_feature_matrix(...): Blend embeddings, TF-IDF, keyword axes, and stats into a feature space.
"""

from __future__ import annotations

import os

# numba coverage hook fails on Windows when coverage.types.Tracer is missing
os.environ.setdefault("NUMBA_DISABLE_COVERAGE", "1")

# coverage shim start
try:
    import coverage
except Exception:
    coverage = None
else:
    try:
        _coverage_types = coverage.types
    except AttributeError:
        _coverage_types = None
    if _coverage_types is not None:
        from typing import Callable, Any
        from types import FrameType

        if not hasattr(_coverage_types, 'Tracer'):
            class _CoverageTracerStub:
                ...

            _coverage_types.Tracer = _CoverageTracerStub
        if not hasattr(_coverage_types, 'TTraceFn'):
            _coverage_types.TTraceFn = Callable[[FrameType, str, Any], Any]
        if not hasattr(_coverage_types, 'TTraceData'):
            _coverage_types.TTraceData = dict[str, set[int]]
        if not hasattr(_coverage_types, 'TShouldTraceFn'):
            _coverage_types.TShouldTraceFn = Callable[[str, FrameType], Any]
        if not hasattr(_coverage_types, 'TFileDisposition'):
            class _FileDispositionStub:
                canonical_filename: str = ''
                file_tracer = None

            _coverage_types.TFileDisposition = _FileDispositionStub
        if not hasattr(_coverage_types, 'TShouldStartContextFn'):
            _coverage_types.TShouldStartContextFn = Callable[[FrameType], str | None]
        if not hasattr(_coverage_types, 'TWarnFn'):
            _coverage_types.TWarnFn = Callable[[str], None]
# coverage shim end

from dataclasses import dataclass
from typing import Optional, Sequence

import numpy as np
import umap
from hdbscan import HDBSCAN
from numpy.typing import ArrayLike
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_samples


@dataclass(slots=True)
class ProjectionResult:
    coords_3d: np.ndarray
    coords_2d: np.ndarray
    method: str = "umap"


@dataclass(slots=True)
class ClusterResult:
    labels: np.ndarray
    probabilities: np.ndarray
    centroid_embeddings: dict[int, np.ndarray]
    centroid_xyz: dict[int, np.ndarray]
    average_similarities: dict[int, float]
    per_point_similarity: np.ndarray
    outlier_scores: np.ndarray
    silhouette_scores: np.ndarray
    method: str = "hdbscan"


def compute_umap(
    embeddings: ArrayLike,
    *,
    random_state: int = 42,
    n_neighbors: int = 15,
    min_dist: float = 0.1,
    metric: str = "cosine",
) -> ProjectionResult:
    matrix = np.asarray(embeddings, dtype=float)
    count = matrix.shape[0]
    if count == 0:
        return ProjectionResult(
            coords_3d=np.zeros((0, 3), dtype=float),
            coords_2d=np.zeros((0, 2), dtype=float),
        )

    if count == 1:
        return ProjectionResult(
            coords_3d=np.zeros((1, 3), dtype=float),
            coords_2d=np.zeros((1, 2), dtype=float),
        )

    if count == 2:
        base = np.array([[-0.5, 0.0, 0.0], [0.5, 0.0, 0.0]], dtype=float)
        return ProjectionResult(coords_3d=base, coords_2d=base[:, :2])

    if count == 3:
        base = np.array(
            [
                [0.0, 1.0, 0.0],
                [-0.8660254, -0.5, 0.0],
                [0.8660254, -0.5, 0.0],
            ],
            dtype=float,
        )
        return ProjectionResult(coords_3d=base, coords_2d=base[:, :2])

    neighbours = max(2, min(n_neighbors, count - 1))

    reducer3d = umap.UMAP(
        n_components=3,
        n_neighbors=neighbours,
        min_dist=min_dist,
        metric=metric,
        random_state=random_state,
    )
    coords_3d = reducer3d.fit_transform(matrix)

    reducer2d = umap.UMAP(
        n_components=2,
        n_neighbors=neighbours,
        min_dist=min_dist,
        metric=metric,
        random_state=random_state,
    )
    coords_2d = reducer2d.fit_transform(matrix)

    return ProjectionResult(coords_3d=coords_3d, coords_2d=coords_2d)


def run_hdbscan(
    embeddings: ArrayLike,
    coords_3d: Optional[np.ndarray] = None,
    *,
    min_cluster_size: int = 5,
    min_samples: int = 1,
    metric: str = "euclidean",
    similarity_basis: Optional[ArrayLike] = None,
) -> ClusterResult:
    matrix = np.asarray(embeddings, dtype=float)
    count = matrix.shape[0]
    if count == 0:
        empty = np.array([], dtype=int)
        zeros = np.array([], dtype=float)
        return ClusterResult(
            labels=empty,
            probabilities=zeros,
            centroid_embeddings={},
            centroid_xyz={},
            average_similarities={},
            per_point_similarity=zeros,
            outlier_scores=zeros,
            silhouette_scores=zeros,
        )

    if count < max(2, min_cluster_size):
        labels = np.full(count, -1, dtype=int)
        zeros = np.zeros(count, dtype=float)
        return ClusterResult(
            labels=labels,
            probabilities=zeros,
            centroid_embeddings={},
            centroid_xyz={},
            average_similarities={},
            per_point_similarity=zeros,
            outlier_scores=zeros,
            silhouette_scores=zeros,
        )

    clusterer = HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric=metric,
        prediction_data=True,
    )
    labels = clusterer.fit_predict(matrix)
    probabilities = clusterer.probabilities_
    outlier_scores = getattr(clusterer, "outlier_scores_", np.zeros(count, dtype=float))

    centroid_embeddings: dict[int, np.ndarray] = {}
    centroid_xyz: dict[int, np.ndarray] = {}
    average_similarities: dict[int, float] = {}
    per_point_similarity = np.zeros(count, dtype=float)

    similarity_vectors = (
        np.asarray(similarity_basis, dtype=float)
        if similarity_basis is not None
        else matrix
    )
    similarity_vectors = _l2_normalise(similarity_vectors.astype(float))

    for label in np.unique(labels[labels >= 0]):
        indices = np.where(labels == label)[0]
        vectors = similarity_vectors[indices]
        centroid = _safe_normalise(vectors.mean(axis=0)) if vectors.size else np.zeros(vectors.shape[1], dtype=float)
        centroid_embeddings[label] = centroid
        if coords_3d is not None and coords_3d.size:
            centroid_coords = coords_3d[indices]
            coords_slice = centroid_coords[:, :3]
            if coords_slice.shape[1] < 3:
                coords_slice = np.pad(coords_slice, ((0, 0), (0, 3 - coords_slice.shape[1])), constant_values=0.0)
            centroid_xyz[label] = coords_slice.mean(axis=0)
        sims = np.clip(vectors @ centroid, -1.0, 1.0)
        per_point_similarity[indices] = sims
        average_similarities[label] = float(np.mean(sims)) if sims.size else 0.0

    silhouette_scores = _compute_silhouette(matrix, labels, metric)

    return ClusterResult(
        labels=labels,
        probabilities=probabilities,
        centroid_embeddings=centroid_embeddings,
        centroid_xyz=centroid_xyz,
        average_similarities=average_similarities,
        per_point_similarity=per_point_similarity,
        outlier_scores=outlier_scores,
        silhouette_scores=silhouette_scores,
        method="hdbscan",
    )


def run_kmeans(
    embeddings: ArrayLike,
    coords_3d: Optional[np.ndarray] = None,
    *,
    n_clusters: int,
    similarity_basis: Optional[ArrayLike] = None,
) -> ClusterResult:
    matrix = np.asarray(embeddings, dtype=float)
    if matrix.shape[0] == 0:
        empty = np.array([], dtype=int)
        zeros = np.array([], dtype=float)
        return ClusterResult(
            labels=empty,
            probabilities=zeros,
            centroid_embeddings={},
            centroid_xyz={},
            average_similarities={},
            per_point_similarity=zeros,
            outlier_scores=zeros,
            silhouette_scores=zeros,
            method="kmeans",
        )

    km = KMeans(n_clusters=n_clusters, n_init="auto", random_state=42)
    labels = km.fit_predict(matrix)

    distances = km.transform(matrix)
    min_distances = distances[np.arange(distances.shape[0]), labels]
    max_distance = np.max(min_distances) or 1.0
    probabilities = 1.0 - (min_distances / max_distance)

    centroid_embeddings: dict[int, np.ndarray] = {}
    centroid_xyz: dict[int, np.ndarray] = {}
    average_similarities: dict[int, float] = {}
    per_point_similarity = np.zeros(matrix.shape[0], dtype=float)

    similarity_vectors = (
        np.asarray(similarity_basis, dtype=float)
        if similarity_basis is not None
        else matrix
    )
    similarity_vectors = _l2_normalise(similarity_vectors.astype(float))

    for label in range(n_clusters):
        indices = np.where(labels == label)[0]
        if indices.size == 0:
            continue
        vectors = similarity_vectors[indices]
        centroid = _safe_normalise(vectors.mean(axis=0))
        centroid_embeddings[label] = centroid
        if coords_3d is not None and coords_3d.size:
            centroid_coords = coords_3d[indices]
            coords_slice = centroid_coords[:, :3]
            if coords_slice.shape[1] < 3:
                coords_slice = np.pad(coords_slice, ((0, 0), (0, 3 - coords_slice.shape[1])), constant_values=0.0)
            centroid_xyz[label] = coords_slice.mean(axis=0)
        sims = np.clip(vectors @ centroid, -1.0, 1.0)
        per_point_similarity[indices] = sims
        average_similarities[label] = float(np.mean(sims)) if sims.size else 0.0

    silhouette_scores = _compute_silhouette(matrix, labels, "euclidean")

    return ClusterResult(
        labels=labels,
        probabilities=probabilities.astype(float),
        centroid_embeddings=centroid_embeddings,
        centroid_xyz=centroid_xyz,
        average_similarities=average_similarities,
        per_point_similarity=per_point_similarity,
        outlier_scores=np.zeros(matrix.shape[0], dtype=float),
        silhouette_scores=silhouette_scores,
        method="kmeans",
    )


def cluster_with_fallback(
    feature_matrix: ArrayLike,
    *,
    coords_3d: Optional[np.ndarray] = None,
    similarity_basis: Optional[ArrayLike] = None,
    min_cluster_size: int = 5,
    min_samples: int = 1,
    metric: str = "euclidean",
) -> ClusterResult:
    matrix = np.asarray(feature_matrix, dtype=float)
    count = matrix.shape[0]
    if count <= 1:
        return run_hdbscan(
            matrix,
            coords_3d=coords_3d,
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            similarity_basis=similarity_basis,
        )

    result = run_hdbscan(
        matrix,
        coords_3d=coords_3d,
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric=metric,
        similarity_basis=similarity_basis,
    )

    if result.labels.size and np.all(result.labels < 0) and count >= 3:
        result = run_hdbscan(
            matrix,
            coords_3d=coords_3d,
            min_cluster_size=max(2, min_cluster_size // 2),
            min_samples=1,
            metric=metric,
            similarity_basis=similarity_basis,
        )

    if result.labels.size and np.all(result.labels < 0) and count >= 2:
        n_clusters = _suggest_cluster_count(count)
        result = run_kmeans(
            matrix,
            coords_3d=coords_3d,
            n_clusters=n_clusters,
            similarity_basis=similarity_basis,
        )

    return result


def build_feature_matrix(
    texts: Sequence[str],
    embeddings: ArrayLike,
    *,
    prompt_embedding: Optional[ArrayLike] = None,
    max_features: int = 512,
    tfidf_weight: float = 0.4,
    stats_weight: float = 0.25,
    keyword_axes: Optional[Sequence[str]] = None,
) -> np.ndarray:
    embedding_matrix = np.asarray(embeddings, dtype=float)
    features: list[np.ndarray] = []

    if embedding_matrix.size:
        features.append(embedding_matrix.astype(np.float32))

    if texts:
        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2), min_df=1)
        tfidf = vectorizer.fit_transform(texts).toarray().astype(np.float32)
        if tfidf_weight != 1.0:
            tfidf *= tfidf_weight
        features.append(tfidf)

    if keyword_axes:
        keyword_matrix = _build_keyword_matrix(texts, keyword_axes)
        if keyword_matrix is not None:
            features.append(keyword_matrix.astype(np.float32))

    stats_features: list[np.ndarray] = []
    if embedding_matrix.size:
        normalised = _l2_normalise(embedding_matrix)
        centroid = _safe_normalise(normalised.mean(axis=0))
        centroid_sim = (normalised @ centroid)[:, None]
        centroid_dist = np.linalg.norm(normalised - centroid, axis=1, keepdims=True)
        norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)

        stats_features.extend([
            centroid_sim.astype(np.float32),
            centroid_dist.astype(np.float32),
            norms.astype(np.float32),
        ])

        if prompt_embedding is not None:
            prompt_vec = np.asarray(prompt_embedding, dtype=float)
            if prompt_vec.size:
                prompt_vec = _safe_normalise(prompt_vec)
                prompt_sim = (normalised @ prompt_vec)[:, None]
                stats_features.append(prompt_sim.astype(np.float32))

    if stats_features:
        stats = np.hstack(stats_features)
        if stats_weight != 1.0:
            stats *= stats_weight
        features.append(stats.astype(np.float32))

    if not features:
        rows = len(texts)
        return np.zeros((rows, 0), dtype=np.float32)

    return np.hstack(features)


def _build_keyword_matrix(texts: Sequence[str], keyword_axes: Sequence[str]) -> Optional[np.ndarray]:
    lowered_keywords = [keyword.lower().strip() for keyword in keyword_axes if keyword.strip()]
    if not lowered_keywords:
        return None
    matrix = np.zeros((len(texts), len(lowered_keywords)), dtype=np.float32)
    for row, text in enumerate(texts):
        lowered = text.lower()
        for col, keyword in enumerate(lowered_keywords):
            matrix[row, col] = lowered.count(keyword)
    return matrix


def _compute_silhouette(matrix: np.ndarray, labels: np.ndarray, metric: str) -> np.ndarray:
    count = matrix.shape[0]
    scores = np.zeros(count, dtype=float)
    if count < 2:
        return scores
    mask = labels >= 0
    labelled = labels[mask]
    if mask.sum() < 2 or len(np.unique(labelled)) < 2:
        return scores
    try:
        sil = silhouette_samples(matrix[mask], labelled, metric=metric)
    except Exception:
        return scores
    scores[mask] = sil
    return scores


def _l2_normalise(matrix: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(matrix, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    return matrix / norms


def _safe_normalise(vector: np.ndarray) -> np.ndarray:
    norm = np.linalg.norm(vector)
    if norm == 0:
        return vector
    return vector / norm


def _suggest_cluster_count(count: int) -> int:
    if count <= 2:
        return count
    return max(2, min(8, int(np.sqrt(count)) or 2))


def _suggest_hdbscan_epsilon(count: int) -> float:
    if count <= 10:
        return 0.08
    if count <= 30:
        return 0.05
    if count <= 80:
        return 0.02
    return 0.0

