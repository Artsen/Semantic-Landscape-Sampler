"""Projection and clustering utilities for semantic landscapes.































Classes:















    ProjectionResult: Holds paired 2D/3D coordinate matrices generated by dimensionality reduction.















    ClusterResult: Encapsulates clustering outputs including labels, probabilities, similarities, and centroids.































Functions:















    compute_umap(...): Produce normalised 2D and 3D coordinates from embedding matrices.















    run_hdbscan(...)/run_kmeans(...): Perform clustering with probabilistic fallbacks.















    cluster_with_fallback(...): Choose an appropriate clustering strategy based on dataset size.















    build_feature_matrix(...): Blend embeddings, TF-IDF, keyword axes, and stats into a feature space.















"""































from __future__ import annotations































import os































# numba coverage hook fails on Windows when coverage.types.Tracer is missing















os.environ.setdefault("NUMBA_DISABLE_COVERAGE", "1")

FEATURE_VERSION = "blend-v1"































# coverage shim start















try:















    import coverage















except Exception:















    coverage = None















else:















    try:















        _coverage_types = coverage.types















    except AttributeError:















        _coverage_types = None















    if _coverage_types is not None:















        from typing import Callable, Any















        from types import FrameType































        if not hasattr(_coverage_types, 'Tracer'):















            class _CoverageTracerStub:















                ...































            _coverage_types.Tracer = _CoverageTracerStub















        if not hasattr(_coverage_types, 'TTraceFn'):















            _coverage_types.TTraceFn = Callable[[FrameType, str, Any], Any]















        if not hasattr(_coverage_types, 'TTraceData'):















            _coverage_types.TTraceData = dict[str, set[int]]















        if not hasattr(_coverage_types, 'TShouldTraceFn'):















            _coverage_types.TShouldTraceFn = Callable[[str, FrameType], Any]















        if not hasattr(_coverage_types, 'TFileDisposition'):















            class _FileDispositionStub:















                canonical_filename: str = ''















                file_tracer = None































            _coverage_types.TFileDisposition = _FileDispositionStub















        if not hasattr(_coverage_types, 'TShouldStartContextFn'):















            _coverage_types.TShouldStartContextFn = Callable[[FrameType], str | None]















        if not hasattr(_coverage_types, 'TWarnFn'):















            _coverage_types.TWarnFn = Callable[[str], None]















# coverage shim end































from dataclasses import dataclass















from typing import Any, Optional, Sequence































import numpy as np















import math















import umap















from hdbscan import HDBSCAN















from numpy.typing import ArrayLike















from sklearn.cluster import KMeans















from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA















from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score, calinski_harabasz_score















from sklearn.manifold import TSNE, trustworthiness















from sklearn.neighbors import NearestNeighbors















































@dataclass(slots=True)















class ProjectionResult:















    coords_3d: np.ndarray















    coords_2d: np.ndarray















    method: str = "umap"















    trustworthiness_3d: Optional[float] = None















    trustworthiness_2d: Optional[float] = None















    continuity_3d: Optional[float] = None















    continuity_2d: Optional[float] = None















































@dataclass(slots=True)















class ClusterResult:















    labels: np.ndarray















    probabilities: np.ndarray















    centroid_embeddings: dict[int, np.ndarray]















    centroid_xyz: dict[int, np.ndarray]















    average_similarities: dict[int, float]















    per_point_similarity: np.ndarray















    outlier_scores: np.ndarray















    silhouette_scores: np.ndarray















    method: str = "hdbscan"















    metadata: dict[str, Any] | None = None















































def compute_umap(















    embeddings: ArrayLike,















    *,















    random_state: int = 42,















    n_neighbors: int = 15,















    min_dist: float = 0.1,















    metric: str = "cosine",















) -> ProjectionResult:















    matrix = np.asarray(embeddings, dtype=float)















    count = matrix.shape[0]















    if count == 0:















        return ProjectionResult(















            coords_3d=np.zeros((0, 3), dtype=float),















            coords_2d=np.zeros((0, 2), dtype=float),















        )































    if count == 1:















        return ProjectionResult(















            coords_3d=np.zeros((1, 3), dtype=float),















            coords_2d=np.zeros((1, 2), dtype=float),















        )































    if count == 2:















        base = np.array([[-0.5, 0.0, 0.0], [0.5, 0.0, 0.0]], dtype=float)















        return ProjectionResult(coords_3d=base, coords_2d=base[:, :2])































    if count == 3:















        base = np.array(















            [















                [0.0, 1.0, 0.0],















                [-0.8660254, -0.5, 0.0],















                [0.8660254, -0.5, 0.0],















            ],















            dtype=float,















        )















        return ProjectionResult(coords_3d=base, coords_2d=base[:, :2])































    neighbours = max(2, min(n_neighbors, count - 1))































    reducer3d = umap.UMAP(















        n_components=3,















        n_neighbors=neighbours,















        min_dist=min_dist,















        metric=metric,















        random_state=random_state,















    )















    coords_3d = reducer3d.fit_transform(matrix)































    reducer2d = umap.UMAP(















        n_components=2,















        n_neighbors=neighbours,















        min_dist=min_dist,















        metric=metric,















        random_state=random_state,















    )















    coords_2d = reducer2d.fit_transform(matrix)































    metrics = _compute_umap_quality(















        matrix,















        coords_2d,















        coords_3d,















        neighbours,















        metric,















    )































    return ProjectionResult(















        coords_3d=coords_3d,















        coords_2d=coords_2d,















        trustworthiness_3d=metrics.get("trustworthiness_3d"),















        trustworthiness_2d=metrics.get("trustworthiness_2d"),















        continuity_3d=metrics.get("continuity_3d"),















        continuity_2d=metrics.get("continuity_2d"),















    )















































def run_hdbscan(















    embeddings: ArrayLike,















    coords_3d: Optional[np.ndarray] = None,















    *,















    min_cluster_size: int = 5,















    min_samples: int = 1,















    metric: str = "euclidean",















    similarity_basis: Optional[ArrayLike] = None,















) -> ClusterResult:















    matrix = np.asarray(embeddings, dtype=float)















    count = matrix.shape[0]















    if count == 0:















        empty = np.array([], dtype=int)















        zeros = np.array([], dtype=float)















        return ClusterResult(















            labels=empty,















            probabilities=zeros,















            centroid_embeddings={},















            centroid_xyz={},















            average_similarities={},















            per_point_similarity=zeros,















            outlier_scores=zeros,















            silhouette_scores=zeros,















            metadata=None,















        )































    if count < max(2, min_cluster_size):















        labels = np.full(count, -1, dtype=int)















        zeros = np.zeros(count, dtype=float)















        return ClusterResult(















            labels=labels,















            probabilities=zeros,















            centroid_embeddings={},















            centroid_xyz={},















            average_similarities={},















            per_point_similarity=zeros,















            outlier_scores=zeros,















            silhouette_scores=zeros,















            metadata=None,















        )































    clusterer = HDBSCAN(















        min_cluster_size=min_cluster_size,















        min_samples=min_samples,















        metric=metric,















        prediction_data=True,















    )















    labels = clusterer.fit_predict(matrix)















    probabilities = clusterer.probabilities_















    outlier_scores = getattr(clusterer, "outlier_scores_", np.zeros(count, dtype=float))































    centroid_embeddings: dict[int, np.ndarray] = {}















    cluster_metadata: dict[str, Any] | None = None















    if hasattr(clusterer, "cluster_persistence_"):















        try:















            unique_labels = sorted({int(label) for label in labels if int(label) >= 0})















            persistences = list(getattr(clusterer, "cluster_persistence_", []))















            persistence_map: dict[int, float] = {}















            for idx_label, label in enumerate(unique_labels):















                if idx_label < len(persistences):















                    persistence_map[label] = float(persistences[idx_label])















            if persistence_map:















                cluster_metadata = {"persistence": persistence_map}















        except Exception:















            cluster_metadata = None































    centroid_xyz: dict[int, np.ndarray] = {}















    average_similarities: dict[int, float] = {}















    per_point_similarity = np.zeros(count, dtype=float)































    similarity_vectors = (















        np.asarray(similarity_basis, dtype=float)















        if similarity_basis is not None















        else matrix















    )















    similarity_vectors = _l2_normalise(similarity_vectors.astype(float))































    for label in np.unique(labels[labels >= 0]):















        indices = np.where(labels == label)[0]















        vectors = similarity_vectors[indices]















        centroid = _safe_normalise(vectors.mean(axis=0)) if vectors.size else np.zeros(vectors.shape[1], dtype=float)















        centroid_embeddings[label] = centroid















        if coords_3d is not None and coords_3d.size:















            centroid_coords = coords_3d[indices]















            coords_slice = centroid_coords[:, :3]















            if coords_slice.shape[1] < 3:















                coords_slice = np.pad(coords_slice, ((0, 0), (0, 3 - coords_slice.shape[1])), constant_values=0.0)















            centroid_xyz[label] = coords_slice.mean(axis=0)















        sims = np.clip(vectors @ centroid, -1.0, 1.0)















        per_point_similarity[indices] = sims















        average_similarities[label] = float(np.mean(sims)) if sims.size else 0.0































    silhouette_scores = _compute_silhouette(matrix, labels, metric)































    return ClusterResult(















        labels=labels,















        probabilities=probabilities,















        centroid_embeddings=centroid_embeddings,















        centroid_xyz=centroid_xyz,















        average_similarities=average_similarities,















        per_point_similarity=per_point_similarity,















        outlier_scores=outlier_scores,















        silhouette_scores=silhouette_scores,















        method="hdbscan",















    )















































def run_kmeans(















    embeddings: ArrayLike,















    coords_3d: Optional[np.ndarray] = None,















    *,















    n_clusters: int,















    similarity_basis: Optional[ArrayLike] = None,















) -> ClusterResult:















    matrix = np.asarray(embeddings, dtype=float)















    if matrix.shape[0] == 0:















        empty = np.array([], dtype=int)















        zeros = np.array([], dtype=float)















        return ClusterResult(















            labels=empty,















            probabilities=zeros,















            centroid_embeddings={},















            centroid_xyz={},















            average_similarities={},















            per_point_similarity=zeros,















            outlier_scores=zeros,















            silhouette_scores=zeros,















            method="kmeans",















            metadata=None,















        )































    km = KMeans(n_clusters=n_clusters, n_init="auto", random_state=42)















    labels = km.fit_predict(matrix)































    distances = km.transform(matrix)















    min_distances = distances[np.arange(distances.shape[0]), labels]















    max_distance = np.max(min_distances) or 1.0















    probabilities = 1.0 - (min_distances / max_distance)































    centroid_embeddings: dict[int, np.ndarray] = {}















    centroid_xyz: dict[int, np.ndarray] = {}















    average_similarities: dict[int, float] = {}















    per_point_similarity = np.zeros(matrix.shape[0], dtype=float)































    similarity_vectors = (















        np.asarray(similarity_basis, dtype=float)















        if similarity_basis is not None















        else matrix















    )















    similarity_vectors = _l2_normalise(similarity_vectors.astype(float))































    for label in range(n_clusters):















        indices = np.where(labels == label)[0]















        if indices.size == 0:















            continue















        vectors = similarity_vectors[indices]















        centroid = _safe_normalise(vectors.mean(axis=0))















        centroid_embeddings[label] = centroid















        if coords_3d is not None and coords_3d.size:















            centroid_coords = coords_3d[indices]















            coords_slice = centroid_coords[:, :3]















            if coords_slice.shape[1] < 3:















                coords_slice = np.pad(coords_slice, ((0, 0), (0, 3 - coords_slice.shape[1])), constant_values=0.0)















            centroid_xyz[label] = coords_slice.mean(axis=0)















        sims = np.clip(vectors @ centroid, -1.0, 1.0)















        per_point_similarity[indices] = sims















        average_similarities[label] = float(np.mean(sims)) if sims.size else 0.0































    silhouette_scores = _compute_silhouette(matrix, labels, "euclidean")































    return ClusterResult(















        labels=labels,















        probabilities=probabilities.astype(float),















        centroid_embeddings=centroid_embeddings,















        centroid_xyz=centroid_xyz,















        average_similarities=average_similarities,















        per_point_similarity=per_point_similarity,















        outlier_scores=np.zeros(matrix.shape[0], dtype=float),















        silhouette_scores=silhouette_scores,















        method="kmeans",















        metadata=None,















    )





























































def cluster_with_fallback(

    feature_matrix: ArrayLike,

    *,

    coords_3d: Optional[np.ndarray] = None,

    similarity_basis: Optional[ArrayLike] = None,

    min_cluster_size: int = 5,

    min_samples: int = 1,

    metric: str = "euclidean",

    algo: str = "hdbscan",

) -> ClusterResult:

    matrix = np.asarray(feature_matrix, dtype=float)

    count = matrix.shape[0]



    if algo == "kmeans":

        if count == 0:

            empty = np.array([], dtype=int)

            zeros = np.array([], dtype=float)

            return ClusterResult(

                labels=empty,

                probabilities=zeros,

                centroid_embeddings={},

                centroid_xyz={},

                average_similarities={},

                per_point_similarity=zeros,

                outlier_scores=zeros,

                silhouette_scores=zeros,

                method="kmeans",

                metadata=None,

            )

        n_clusters = max(1, _suggest_cluster_count(count))

        return run_kmeans(

            matrix,

            coords_3d=coords_3d,

            n_clusters=n_clusters,

            similarity_basis=similarity_basis,

        )



    if count <= 1:

        return run_hdbscan(

            matrix,

            coords_3d=coords_3d,

            min_cluster_size=min_cluster_size,

            min_samples=min_samples,

            similarity_basis=similarity_basis,

        )



    result = run_hdbscan(

        matrix,

        coords_3d=coords_3d,

        min_cluster_size=min_cluster_size,

        min_samples=min_samples,

        metric=metric,

        similarity_basis=similarity_basis,

    )



    if result.labels.size and np.all(result.labels < 0) and count >= 3:

        result = run_hdbscan(

            matrix,

            coords_3d=coords_3d,

            min_cluster_size=max(2, min_cluster_size // 2),

            min_samples=1,

            metric=metric,

            similarity_basis=similarity_basis,

        )



    if result.labels.size and np.all(result.labels < 0) and count >= 2:

        n_clusters = _suggest_cluster_count(count)

        result = run_kmeans(

            matrix,

            coords_3d=coords_3d,

            n_clusters=n_clusters,

            similarity_basis=similarity_basis,

        )



    return result



def build_feature_matrix(















    texts: Sequence[str],















    embeddings: ArrayLike,















    *,















    prompt_embedding: Optional[ArrayLike] = None,















    max_features: int = 512,















    tfidf_weight: float = 0.4,















    stats_weight: float = 0.25,















    keyword_axes: Optional[Sequence[str]] = None,















) -> np.ndarray:















    embedding_matrix = np.asarray(embeddings, dtype=float)















    features: list[np.ndarray] = []































    if embedding_matrix.size:















        features.append(embedding_matrix.astype(np.float32))































    if texts:















        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2), min_df=1)















        tfidf = vectorizer.fit_transform(texts).toarray().astype(np.float32)















        if tfidf_weight != 1.0:















            tfidf *= tfidf_weight















        features.append(tfidf)































    if keyword_axes:















        keyword_matrix = _build_keyword_matrix(texts, keyword_axes)















        if keyword_matrix is not None:















            features.append(keyword_matrix.astype(np.float32))































    stats_features: list[np.ndarray] = []















    if embedding_matrix.size:















        normalised = _l2_normalise(embedding_matrix)















        centroid = _safe_normalise(normalised.mean(axis=0))















        centroid_sim = (normalised @ centroid)[:, None]















        centroid_dist = np.linalg.norm(normalised - centroid, axis=1, keepdims=True)















        norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)































        stats_features.extend([















            centroid_sim.astype(np.float32),















            centroid_dist.astype(np.float32),















            norms.astype(np.float32),















        ])































        if prompt_embedding is not None:















            prompt_vec = np.asarray(prompt_embedding, dtype=float)















            if prompt_vec.size:















                prompt_vec = _safe_normalise(prompt_vec)















                prompt_sim = (normalised @ prompt_vec)[:, None]















                stats_features.append(prompt_sim.astype(np.float32))































    if stats_features:















        stats = np.hstack(stats_features)















        if stats_weight != 1.0:















            stats *= stats_weight















        features.append(stats.astype(np.float32))































    if not features:















        rows = len(texts)















        return np.zeros((rows, 0), dtype=np.float32)































    return np.hstack(features)















































def _build_keyword_matrix(texts: Sequence[str], keyword_axes: Sequence[str]) -> Optional[np.ndarray]:















    lowered_keywords = [keyword.lower().strip() for keyword in keyword_axes if keyword.strip()]















    if not lowered_keywords:















        return None















    matrix = np.zeros((len(texts), len(lowered_keywords)), dtype=np.float32)















    for row, text in enumerate(texts):















        lowered = text.lower()















        for col, keyword in enumerate(lowered_keywords):















            matrix[row, col] = lowered.count(keyword)















    return matrix















































def _compute_silhouette(matrix: np.ndarray, labels: np.ndarray, metric: str) -> np.ndarray:















    count = matrix.shape[0]















    scores = np.zeros(count, dtype=float)















    if count < 2:















        return scores















    mask = labels >= 0















    labelled = labels[mask]















    if mask.sum() < 2 or len(np.unique(labelled)) < 2:















        return scores















    try:















        sil = silhouette_samples(matrix[mask], labelled, metric=metric)















    except Exception:















        return scores















    scores[mask] = sil















    return scores















































def _compute_umap_quality(















    original: np.ndarray,















    coords_2d: np.ndarray,















    coords_3d: np.ndarray,















    neighbours: int,















    metric: str,















) -> dict[str, Optional[float]]:















    count = original.shape[0]















    if count < 5:















        return {}































    k = max(5, min(neighbours, count - 1, 15))















    quality: dict[str, Optional[float]] = {}































    try:















        score = trustworthiness(original, coords_2d, n_neighbors=k, metric=metric)















        quality["trustworthiness_2d"] = float(score) if np.isfinite(score) else None















    except Exception:















        quality["trustworthiness_2d"] = None































    try:















        score = trustworthiness(original, coords_3d, n_neighbors=k, metric=metric)















        quality["trustworthiness_3d"] = float(score) if np.isfinite(score) else None















    except Exception:















        quality["trustworthiness_3d"] = None































    try:















        quality["continuity_2d"] = _compute_continuity(original, coords_2d, k, metric)















    except Exception:















        quality["continuity_2d"] = None































    try:















        quality["continuity_3d"] = _compute_continuity(original, coords_3d, k, metric)















    except Exception:















        quality["continuity_3d"] = None































    return quality















































def _compute_continuity(















    original: np.ndarray,















    embedded: np.ndarray,















    k: int,















    metric: str,















) -> Optional[float]:















    n = original.shape[0]















    if n <= k:















        return None































    denominator = n * k * (2 * n - 3 * k - 1)















    if denominator <= 0:















        return None































    high_nn = NearestNeighbors(n_neighbors=n, metric=metric).fit(original)















    high_indices = high_nn.kneighbors(original, return_distance=False)































    low_nn = NearestNeighbors(n_neighbors=n, metric="euclidean").fit(embedded)















    low_indices = low_nn.kneighbors(embedded, return_distance=False)































    high_neighbors = high_indices[:, 1:]















    low_neighbors = low_indices[:, 1:]































    rank_maps = []















    for row in low_neighbors:















        rank_maps.append({node: rank + 1 for rank, node in enumerate(row)})































    discontinuity = 0.0















    for i in range(n):















        high_set = high_neighbors[i, :k]















        low_top = set(low_neighbors[i, :k])















        rank_map = rank_maps[i]















        for neighbor in high_set:















            rank = rank_map.get(neighbor)















            if rank is None:















                continue















            if neighbor not in low_top and rank > k:















                discontinuity += rank - k































    value = 1.0 - (2.0 / denominator) * discontinuity















    return float(max(0.0, min(1.0, value)))















































def _l2_normalise(matrix: np.ndarray) -> np.ndarray:















    norms = np.linalg.norm(matrix, axis=1, keepdims=True)















    norms[norms == 0] = 1.0















    return matrix / norms















































def _safe_normalise(vector: np.ndarray) -> np.ndarray:















    norm = np.linalg.norm(vector)















    if norm == 0:















        return vector















    return vector / norm















































def _suggest_cluster_count(count: int) -> int:















    if count <= 2:















        return count















    return max(2, min(8, int(np.sqrt(count)) or 2))















































def _suggest_hdbscan_epsilon(count: int) -> float:















    if count <= 10:















        return 0.08















    if count <= 30:















        return 0.05















    if count <= 80:















        return 0.02















    return 0.0

































def compute_tsne(
    features: ArrayLike,
    *,
    random_state: int = 42,
    perplexity: float = 30.0,
    learning_rate: float | str = "auto",
    n_iter: int = 1000,
    init_2d: np.ndarray | None = None,
    init_3d: np.ndarray | None = None,
    early_exaggeration: float = 12.0,
    metric: str = "euclidean",
) -> ProjectionResult:
    """Compute t-SNE projections with optional initial layouts for 2D/3D."""

    matrix = np.asarray(features, dtype=float)
    if matrix.ndim == 1:
        matrix = matrix.reshape(-1, 1)
    count = matrix.shape[0]

    if count == 0:
        return ProjectionResult(
            coords_3d=np.zeros((0, 3), dtype=float),
            coords_2d=np.zeros((0, 2), dtype=float),
            method="tsne",
        )

    if count == 1:
        zeros3 = np.zeros((1, 3), dtype=float)
        return ProjectionResult(coords_3d=zeros3, coords_2d=zeros3[:, :2], method="tsne")

    if count == 2:
        base = np.array([[-0.5, 0.0, 0.0], [0.5, 0.0, 0.0]], dtype=float)
        return ProjectionResult(coords_3d=base, coords_2d=base[:, :2], method="tsne")

    if count == 3:
        base = np.array(
            [[0.0, 1.0, 0.0], [-0.8660254, -0.5, 0.0], [0.8660254, -0.5, 0.0]],
            dtype=float,
        )
        return ProjectionResult(coords_3d=base, coords_2d=base[:, :2], method="tsne")

    max_perplexity = max(5.0, min(perplexity, max(2.0, (count - 1) / 3.0)))
    effective_perplexity = float(min(perplexity, max_perplexity))
    if effective_perplexity >= count:
        effective_perplexity = float(max(2.0, count - 1))

    def _resolve_init(init_array: np.ndarray | None, dim: int) -> str | np.ndarray:
        if init_array is None:
            return "pca"
        arr = np.asarray(init_array, dtype=float)
        if arr.shape != (count, dim):
            return "pca"
        if not np.isfinite(arr).all():
            return "pca"
        return arr

    base_kwargs: dict[str, Any] = {
        "perplexity": effective_perplexity,
        "learning_rate": learning_rate,
        "n_iter": n_iter,
        "early_exaggeration": early_exaggeration,
        "random_state": random_state,
    }

    tsne2 = TSNE(n_components=2, init=_resolve_init(init_2d, 2), **base_kwargs)
    coords_2d = tsne2.fit_transform(matrix)

    tsne3 = TSNE(n_components=3, init=_resolve_init(init_3d, 3), **base_kwargs)
    coords_3d = tsne3.fit_transform(matrix)

    metrics = _compute_umap_quality(matrix, coords_2d, coords_3d, min(15, count - 1), metric)
    return ProjectionResult(
        coords_3d=coords_3d,
        coords_2d=coords_2d,
        method="tsne",
        trustworthiness_3d=metrics.get("trustworthiness_3d"),
        trustworthiness_2d=metrics.get("trustworthiness_2d"),
        continuity_3d=metrics.get("continuity_3d"),
        continuity_2d=metrics.get("continuity_2d"),
    )


def compute_pca_projection(
    features: ArrayLike,
    *,
    n_components: int = 50,
    metric: str = "euclidean",
) -> ProjectionResult:
    """Compute PCA-based 2D/3D projections."""

    matrix = np.asarray(features, dtype=float)
    if matrix.ndim == 1:
        matrix = matrix.reshape(-1, 1)
    count, dims = matrix.shape

    if count == 0:
        return ProjectionResult(
            coords_3d=np.zeros((0, 3), dtype=float),
            coords_2d=np.zeros((0, 2), dtype=float),
            method="pca",
        )

    if count == 1:
        zeros3 = np.zeros((1, 3), dtype=float)
        return ProjectionResult(coords_3d=zeros3, coords_2d=zeros3[:, :2], method="pca")

    if dims == 0:
        zeros3 = np.zeros((count, 3), dtype=float)
        return ProjectionResult(coords_3d=zeros3, coords_2d=zeros3[:, :2], method="pca")

    max_components = max(1, min(n_components, dims, count))
    reducer = PCA(n_components=max_components)
    projected = reducer.fit_transform(matrix)

    if projected.shape[1] < 3:
        pad_width = 3 - projected.shape[1]
        coords_3d = np.pad(projected, ((0, 0), (0, pad_width)), mode="constant")
    else:
        coords_3d = projected[:, :3]

    if projected.shape[1] < 2:
        pad_width = 2 - projected.shape[1]
        coords_2d = np.pad(projected, ((0, 0), (0, pad_width)), mode="constant")[:, :2]
    else:
        coords_2d = projected[:, :2]

    metrics = _compute_umap_quality(matrix, coords_2d, coords_3d, min(15, count - 1), metric)
    return ProjectionResult(
        coords_3d=coords_3d,
        coords_2d=coords_2d,
        method="pca",
        trustworthiness_3d=metrics.get("trustworthiness_3d"),
        trustworthiness_2d=metrics.get("trustworthiness_2d"),
        continuity_3d=metrics.get("continuity_3d"),
        continuity_2d=metrics.get("continuity_2d"),
    )
